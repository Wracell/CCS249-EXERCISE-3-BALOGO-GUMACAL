{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE 3 BALOGO-GUMACAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 1: Install & Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.8.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from wikipedia-api) (2.32.3)\n",
      "Requirement already satisfied: click in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->wikipedia-api) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->wikipedia-api) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->wikipedia-api) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kengu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->wikipedia-api) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kengu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install wikipedia-api nltk\n",
    "\n",
    "# Import necessary libraries\n",
    "import wikipediaapi\n",
    "import nltk\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(10 points) Use the Wikipedia python module and access any topic, as you will use that as your corpus, with a word limit of 1000 words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Fetch Wikipedia Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called A\n"
     ]
    }
   ],
   "source": [
    "# Initialize Wikipedia API\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent=\"Colab-Perplexity\", language='en')\n",
    "\n",
    "# Fetch Wikipedia content for \"Artificial Intelligence\"\n",
    "page = wiki_wiki.page(\"Artificial Intelligence\")\n",
    "text = page.text[:10000]  # Limiting text for performance\n",
    "\n",
    "# Print first 500 characters for verification\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(60 points) Train 2 models: a Bigram and Trigram Language Model, use the shared code as reference for bigram modeling, and update it to support trigrams.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Preprocess Text (Remove Punctuation & Lowercase)\n",
    "\n",
    "Tokenizing the text into words using NLTK.\n",
    "Converting to lowercase.\n",
    "Removing special characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial intelligence ai refers to the capability of computational systems to perform tasks typically associated with human intelligence such as learning reasoning problemsolving perception and decisionmaking it is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals such machines may be called ais\n",
      "highprof\n",
      "['artificial', 'intelligence', 'ai', 'refers', 'to', 'the', 'capability', 'of', 'computational', 'systems', 'to', 'perform', 'tasks', 'typically', 'associated', 'with', 'human', 'intelligence', 'such', 'as']\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation using regex and convert to lowercase\n",
    "text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "\n",
    "# Print cleaned text preview\n",
    "print(text[:500])\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Print first 20 tokens\n",
    "print(tokens[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Generate N-grams (Bigrams & Trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample bigram: [('artificial', 'intelligence'), ('intelligence', 'ai'), ('ai', 'refers'), ('refers', 'to'), ('to', 'the')]\n",
      "Sample trigram: [('artificial', 'intelligence', 'ai'), ('intelligence', 'ai', 'refers'), ('ai', 'refers', 'to'), ('refers', 'to', 'the'), ('to', 'the', 'capability')]\n"
     ]
    }
   ],
   "source": [
    "# Create bigrams and trigrams\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "\n",
    "# Print sample bigram and trigram\n",
    "print(\"Sample bigram:\", bigrams[:5])\n",
    "print(\"Sample trigram:\", trigrams[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Compute N-gram Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram count (sample): [('artificial', 4), ('intelligence', 5), ('ai', 25), ('refers', 1), ('to', 27)]\n",
      "Bigram count (sample): [(('artificial', 'intelligence'), 2), (('intelligence', 'ai'), 1), (('ai', 'refers'), 1), (('refers', 'to'), 1), (('to', 'the'), 2)]\n",
      "Trigram count (sample): [(('artificial', 'intelligence', 'ai'), 1), (('intelligence', 'ai', 'refers'), 1), (('ai', 'refers', 'to'), 1), (('refers', 'to', 'the'), 1), (('to', 'the', 'capability'), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Count unigrams, bigrams, and trigrams\n",
    "unigram_counts = Counter(tokens)\n",
    "bigram_counts = Counter(bigrams)\n",
    "trigram_counts = Counter(trigrams)\n",
    "\n",
    "# Print sample counts\n",
    "print(\"Unigram count (sample):\", list(unigram_counts.items())[:5])\n",
    "print(\"Bigram count (sample):\", list(bigram_counts.items())[:5])\n",
    "print(\"Trigram count (sample):\", list(trigram_counts.items())[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Step 6: Implement Laplace Smoothing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram probability (Laplace smoothed): 0.004942339373970346\n",
      "Trigram probability (Laplace smoothed): 0.0016474464579901153\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "V = len(set(tokens))\n",
    "\n",
    "# Function to compute probability with Laplace Smoothing\n",
    "def laplace_smoothing(ngram, ngram_counts, lower_order_counts, V, k=1):\n",
    "    \"\"\"\n",
    "    Computes probability using Laplace smoothing.\n",
    "    ngram: The n-gram tuple.\n",
    "    ngram_counts: Frequency count of the n-gram.\n",
    "    lower_order_counts: Frequency count of the lower-order n-gram.\n",
    "    V: Vocabulary size.\n",
    "    k: Smoothing parameter (default = 1).\n",
    "    \"\"\"\n",
    "    ngram_count = ngram_counts[ngram] + k  # Add k smoothing\n",
    "    lower_order_count = lower_order_counts[ngram[:-1]] + (V * k)  # Adjust denominator\n",
    "    return ngram_count / lower_order_count\n",
    "\n",
    "# Test probability calculation\n",
    "sample_bigram = ('artificial', 'intelligence')\n",
    "sample_trigram = ('the', 'artificial', 'intelligence')\n",
    "\n",
    "print(\"Bigram probability (Laplace smoothed):\", laplace_smoothing(sample_bigram, bigram_counts, unigram_counts, V))\n",
    "print(\"Trigram probability (Laplace smoothed):\", laplace_smoothing(sample_trigram, trigram_counts, bigram_counts, V))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(30 points) Using a test sentence “The quick brown fox jumps over the lazy dog near the bank of the river.” OR generate your own test sentence, create a function that will determine the perplexity score for each trained model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Implement Perplexity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Perplexity: 549.7742642081902\n",
      "Trigram Perplexity: 607.2302444894901\n"
     ]
    }
   ],
   "source": [
    "def perplexity(test_sentence, ngram_counts, lower_order_counts, n, V):\n",
    "    \"\"\"\n",
    "    Computes the perplexity of a test sentence.\n",
    "    test_sentence: Input sentence to evaluate.\n",
    "    ngram_counts: Frequency count of the n-grams.\n",
    "    lower_order_counts: Frequency count of lower n-grams.\n",
    "    n: Order of the n-gram model.\n",
    "    V: Vocabulary size.\n",
    "    \"\"\"\n",
    "    # Tokenize test sentence (removes punctuation, converts to lowercase)\n",
    "    test_tokens = nltk.word_tokenize(test_sentence.lower())\n",
    "    test_tokens = [word for word in test_tokens if word.isalnum()]  # Remove any leftover punctuation\n",
    "    test_ngrams = list(ngrams(test_tokens, n))\n",
    "\n",
    "    # Compute log probability sum\n",
    "    log_prob_sum = 0\n",
    "    for ngram in test_ngrams:\n",
    "        prob = laplace_smoothing(ngram, ngram_counts, lower_order_counts, V)\n",
    "        log_prob_sum += math.log(prob)\n",
    "\n",
    "    # Compute perplexity\n",
    "    return math.exp(-log_prob_sum / len(test_ngrams))\n",
    "\n",
    "# Test perplexity function with a simple sentence\n",
    "test_sentence = \"The quick brown fox jumps over the lazy dog near the bank of the river.\"\n",
    "\n",
    "bigram_perplexity = perplexity(test_sentence, bigram_counts, unigram_counts, 2, V)\n",
    "trigram_perplexity = perplexity(test_sentence, trigram_counts, bigram_counts, 3, V)\n",
    "\n",
    "print(f\"Bigram Perplexity: {bigram_perplexity}\")\n",
    "print(f\"Trigram Perplexity: {trigram_perplexity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
